---
description: Guide for using Task Master to manage task-driven development workflows
globs: **/*
alwaysApply: true
---
# Task Master Development Workflow

This guide outlines the typical process for using Task Master to manage software development projects.

## Primary Interaction: MCP Server vs. CLI

Task Master offers two primary ways to interact:

1.  **MCP Server (Recommended for Integrated Tools)**:
    - For AI agents and integrated development environments (like Cursor), interacting via the **MCP server is the preferred method**.
    - The MCP server exposes Task Master functionality through a set of tools (e.g., `get_tasks`, `add_subtask`).
    - This method offers better performance, structured data exchange, and richer error handling compared to CLI parsing.
    - Refer to [`mcp.mdc`](mdc:.cursor/rules/mcp.mdc) for details on the MCP architecture and available tools.
    - A comprehensive list and description of MCP tools and their corresponding CLI commands can be found in [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc).
    - **Restart the MCP server** if core logic in `scripts/modules` or MCP tool/direct function definitions change.

2.  **`task-master` CLI (For Users & Fallback)**:
    - The global `task-master` command provides a user-friendly interface for direct terminal interaction.
    - It can also serve as a fallback if the MCP server is inaccessible or a specific function isn't exposed via MCP.
    - Install globally with `npm install -g task-master-ai` or use locally via `npx task-master-ai ...`.
    - The CLI commands often mirror the MCP tools (e.g., `task-master list` corresponds to `get_tasks`).
    - Refer to [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc) for a detailed command reference.

## Task Generation Interaction Flow

When generating new tasks or subtasks (via `parse_prd`, `add_task`, `expand_task`, or `expand_all`), the AI MUST follow this interactive process:

### Step 1: Ask Clarifying Questions
Before generating any tasks or subtasks, the AI MUST:
- Analyze the request/PRD/task description to identify ambiguities or missing information
- Ask specific, relevant questions that would impact implementation decisions
- Focus on questions that would change the structure, approach, or scope of tasks
- Examples of important clarifying questions:
  - "Which database system should be used for this feature?"
  - "Should this API support pagination, and if so, what type?"
  - "Are there specific security requirements for user authentication?"
  - "Should this feature support multiple languages/localization?"
  - "What error handling approach is preferred for this component?"
  - "Are there performance requirements or constraints to consider?"
- Wait for user responses before proceeding to task generation

### Step 2: Present Task Overview for Approval
After gathering all necessary information, but BEFORE writing to the tasks.json file:
- Generate a complete overview of the proposed tasks and subtasks
- Present this overview in a clear, hierarchical format showing:
  - Main tasks with descriptions
  - Subtasks under each main task
  - Dependencies between tasks
  - Estimated complexity or priority
  - For testing tasks: Clear indication of test type and scenarios
- Ask for user confirmation: "Does this task structure look good? Would you like me to modify anything before creating these tasks?"
- Only proceed to write tasks after receiving explicit approval
- If the user requests changes, revise the overview and present again for approval

### Step 3: Execute Task Generation
Only after user approval:
- Execute the appropriate command to write tasks to tasks.json
- Confirm successful task creation
- Provide next steps or suggestions for getting started

**Important:** This interaction flow applies to ALL task generation operations, including:
- Initial project setup with `parse_prd`
- Adding new tasks with `add_task`
- Expanding tasks into subtasks with `expand_task`
- Bulk expansion with `expand_all`

## Standard Development Workflow Process

-   Start new projects by running `initialize_project` tool / `task-master init` or `parse_prd` / `task-master parse-prd --input='<prd-file.txt>'` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to generate initial tasks.json
    -   **When using `parse_prd`**: Follow the [Task Generation Interaction Flow](mdc:#task-generation-interaction-flow) to clarify requirements and get approval before generating tasks
-   Begin coding sessions with `get_tasks` / `task-master list` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to see current tasks, status, and IDs
-   Determine the next task to work on using `next_task` / `task-master next` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).
-   Analyze task complexity with `analyze_project_complexity` / `task-master analyze-complexity --research` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) before breaking down tasks
-   Review complexity report using `complexity_report` / `task-master complexity-report` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).
-   Select tasks based on dependencies (all marked 'done'), priority level, and ID order
-   Clarify tasks by checking task files in tasks/ directory or asking for user input
-   View specific task details using `get_task` / `task-master show <id>` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to understand implementation requirements
-   Break down complex tasks using `expand_task` / `task-master expand --id=<id> --force --research` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) with appropriate flags like `--force` (to replace existing subtasks) and `--research`.
-   Clear existing subtasks if needed using `clear_subtasks` / `task-master clear-subtasks --id=<id>` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) before regenerating
-   Implement code following task details, dependencies, and project standards
-   Verify tasks according to test strategies before marking as complete (See [`tests.mdc`](mdc:.cursor/rules/tests.mdc))
-   Mark completed tasks with `set_task_status` / `task-master set-status --id=<id> --status=done` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc))
-   Update dependent tasks when implementation differs from original plan using `update` / `task-master update --from=<id> --prompt="..."` or `update_task` / `task-master update-task --id=<id> --prompt="..."` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc))
-   Add new tasks discovered during implementation using `add_task` / `task-master add-task --prompt="..." --research` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).
    -   **When adding tasks**: Follow the [Task Generation Interaction Flow](mdc:#task-generation-interaction-flow) to ensure proper task definition
-   Add new subtasks as needed using `add_subtask` / `task-master add-subtask --parent=<id> --title="..."` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).
-   Append notes or details to subtasks using `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='Add implementation notes here...\nMore details...'` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)).
-   Generate task files with `generate` / `task-master generate` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) after updating tasks.json
-   Maintain valid dependency structure with `add_dependency`/`remove_dependency` tools or `task-master add-dependency`/`remove-dependency` commands, `validate_dependencies` / `task-master validate-dependencies`, and `fix_dependencies` / `task-master fix-dependencies` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) when needed
-   Respect dependency chains and task priorities when selecting work
-   Report progress regularly using `get_tasks` / `task-master list`

## Task Complexity Analysis

-   Run `analyze_project_complexity` / `task-master analyze-complexity --research` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) for comprehensive analysis
-   Review complexity report via `complexity_report` / `task-master complexity-report` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) for a formatted, readable version.
-   Focus on tasks with highest complexity scores (8-10) for detailed breakdown
-   Use analysis results to determine appropriate subtask allocation
-   Note that reports are automatically used by the `expand_task` tool/command

## Task Breakdown Process

-   **IMPORTANT**: Before executing any task expansion, follow the [Task Generation Interaction Flow](mdc:#task-generation-interaction-flow) to gather clarifications and get user approval.
-   Use `expand_task` / `task-master expand --id=<id>`. It automatically uses the complexity report if found, otherwise generates default number of subtasks.
    -   **Crucially, when expanding a task that involves code implementation, subtasks for *writing the tests* for that feature/component MUST be explicitly planned and created at this stage.** Do not defer test planning to later. For example, if expanding "Implement Feature X", generate subtasks for "Implement Feature X logic" AND then proceed to generate granular subtasks for testing.
    -   **Granular Test Scenario Subtasks:** Instead of a single "Write Comprehensive Tests for Feature X" subtask, the you MUST create **individual subtasks for each distinct logical test scenario** required for that feature. Examples of distinct scenarios include:
        -   Testing with valid inputs (the "happy path").
        -   Testing specific error conditions (e.g., invalid input, resource not found).
        -   Testing edge cases (e.g., zero values, empty strings, boundary conditions).
        -   Testing specific business logic branches.
    -   **Test Type Indication in Titles:** Each testing subtask MUST indicate in its title whether it is a unit test or integration test. Use the following format:
        -   `[Unit Test] Feature X - Happy Path Validation`
        -   `[Integration Test] API Endpoint - Error Response Handling`
        -   `[Unit Test] Calculator - Division by Zero Edge Case`
    -   **Required 4-Step Structure for ALL Testing Subtasks:** Each individual testing subtask (for each scenario) MUST include the following four steps in its description or details. This is mandatory and must be followed exactly:
        
        **Step 1 - Implement Tests:**
        - Design and write the specific test(s) for this single scenario
        - Follow project testing conventions and patterns
        - Ensure proper test isolation and setup
        
        **Step 2 - Execute Test & Make Sure It Passes:**
        - Run the test using single-run commands (e.g., `vitest run`, `jest --watchAll=false`)
        - Never use watch mode - tests must execute once and exit
        - For Unit Tests: Debug and fix code/tests iteratively until the test passes
        - **EXCEPTION for Integration Tests**: Currently, integration tests only need to pass linter checks. Do not spend time fixing failing integration tests. This is a temporary exception that will be removed soon.
        
        **Step 3 - Learn New Relevant Things:**
        - Reflect on what was discovered during implementation
        - Document any patterns, gotchas, or insights gained
        - Identify what should be added to `learned-memories.mdc` following the [Memory Guidelines](mdc:rules/memory.mdc#what-qualifies-as-important-to-remember)
        - Focus on high-impact learnings that will affect future development:
          - Architectural patterns that worked well
          - Technical gotchas and their solutions
          - Failed approaches to avoid
          - Business logic rules discovered
        - Consider: What worked? What didn't? What would help future development?
        - Do NOT document trivial details or one-off fixes
        
        **Step 4 - Commit the Code:**
        - Stage all changes related to this test scenario
        - Create a descriptive commit message that references the specific scenario
        - Example: `test(auth): Add unit test for login happy path validation`
        - Example: `test(api): Add integration test for 404 error handling`

-   Use `--num=<number>` to specify an explicit number of subtasks, overriding defaults or complexity report recommendations.
-   Add `--research` flag to leverage Perplexity AI for research-backed expansion.
-   Add `--force` flag to clear existing subtasks before generating new ones (default is to append).
-   Use `--prompt="<context>"` to provide additional context when needed.
-   Review and adjust generated subtasks as necessary, ensuring test creation subtasks are present and correctly scoped.
-   Use `expand_all` tool or `task-master expand --all` to expand multiple pending tasks at once, respecting flags like `--force` and `--research`. This also requires ensuring that corresponding test creation subtasks are planned for each expanded task.
-   If subtasks need complete replacement (regardless of the `--force` flag on `expand`), clear them first with `clear_subtasks` / `task-master clear-subtasks --id=<id>`.

## Implementation Drift Handling

-   When implementation differs significantly from planned approach
-   When future tasks need modification due to current implementation choices
-   When new dependencies or requirements emerge
-   Use `update` / `task-master update --from=<futureTaskId> --prompt='<explanation>\nUpdate context...' --research` to update multiple future tasks.
-   Use `update_task` / `task-master update-task --id=<taskId> --prompt='<explanation>\nUpdate context...' --research` to update a single specific task.

## Task Status Management

-   Use 'pending' for tasks ready to be worked on
-   Use 'done' for completed and verified tasks
-   Use 'deferred' for postponed tasks
-   Add custom status values as needed for project-specific workflows

## Task Structure Fields

- **id**: Unique identifier for the task (Example: `1`, `1.1`)
- **title**: Brief, descriptive title (Example: `"Initialize Repo"`)
- **description**: Concise summary of what the task involves (Example: `"Create a new repository, set up initial structure."`)
- **status**: Current state of the task (Example: `"pending"`, `"done"`, `"deferred"`)
- **dependencies**: IDs of prerequisite tasks (Example: `[1, 2.1]`)
    - Dependencies are displayed with status indicators (✅ for completed, ⏱️ for pending)
    - This helps quickly identify which prerequisite tasks are blocking work
- **priority**: Importance level (Example: `"high"`, `"medium"`, `"low"`)
- **details**: In-depth implementation instructions (Example: `"Use GitHub client ID/secret, handle callback, set session token."`) 
- **testStrategy**: Verification approach (Example: `"Deploy and call endpoint to confirm 'Hello World' response."`) 
- **subtasks**: List of smaller, more specific tasks (Example: `[{"id": 1, "title": "Configure OAuth", ...}]`) 
- Refer to task structure details (previously linked to `tasks.mdc`).

## Configuration Management (Updated)

Taskmaster configuration is managed through two main mechanisms:

1.  **`.taskmasterconfig` File (Primary):**
    *   Located in the project root directory.
    *   Stores most configuration settings: AI model selections (main, research, fallback), parameters (max tokens, temperature), logging level, default subtasks/priority, project name, etc.
    *   **Managed via `task-master models --setup` command.** Do not edit manually unless you know what you are doing.
    *   **View/Set specific models via `task-master models` command or `models` MCP tool.**
    *   Created automatically when you run `task-master models --setup` for the first time.

2.  **Environment Variables (`.env` / `mcp.json`):**
    *   Used **only** for sensitive API keys and specific endpoint URLs.
    *   Place API keys (one per provider) in a `.env` file in the project root for CLI usage.
    *   For MCP/Cursor integration, configure these keys in the `env` section of `.cursor/mcp.json`.
    *   Available keys/variables: See `assets/env.example` or the Configuration section in the command reference (previously linked to `taskmaster.mdc`).

**Important:** Non-API key settings (like model selections, `MAX_TOKENS`, `TASKMASTER_LOG_LEVEL`) are **no longer configured via environment variables**. Use the `task-master models` command (or `--setup` for interactive configuration) or the `models` MCP tool.
**If AI commands FAIL in MCP** verify that the API key for the selected provider is present in the `env` section of `.cursor/mcp.json`.
**If AI commands FAIL in CLI** verify that the API key for the selected provider is present in the `.env` file in the root of the project.

## Determining the Next Task

- Run `next_task` / `task-master next` to show the next task to work on.
- The command identifies tasks with all dependencies satisfied
- Tasks are prioritized by priority level, dependency count, and ID
- The command shows comprehensive task information including:
    - Basic task details and description
    - Implementation details
    - Subtasks (if they exist)
    - Contextual suggested actions
- Recommended before starting any new development work
- Respects your project's dependency structure
- Ensures tasks are completed in the appropriate sequence
- Provides ready-to-use commands for common task actions

## Viewing Specific Task Details

- Run `get_task` / `task-master show <id>` to view a specific task.
- Use dot notation for subtasks: `task-master show 1.2` (shows subtask 2 of task 1)
- Displays comprehensive information similar to the next command, but for a specific task
- For parent tasks, shows all subtasks and their current status
- For subtasks, shows parent task information and relationship
- Provides contextual suggested actions appropriate for the specific task
- Useful for examining task details before implementation or checking status

## Managing Task Dependencies

- Use `add_dependency` / `task-master add-dependency --id=<id> --depends-on=<id>` to add a dependency.
- Use `remove_dependency` / `task-master remove-dependency --id=<id> --depends-on=<id>` to remove a dependency.
- The system prevents circular dependencies and duplicate dependency entries
- Dependencies are checked for existence before being added or removed
- Task files are automatically regenerated after dependency changes
- Dependencies are visualized with status indicators in task listings and files

## Iterative Subtask Implementation

Once a task has been broken down into subtasks using `expand_task` or similar methods, follow this iterative process for implementation:

1.  **Understand the Goal (Preparation):**
    *   Use `get_task` / `task-master show <subtaskId>` (see [`taskmaster.mdc`](mdc:.cursor/rules/taskmaster.mdc)) to thoroughly understand the specific goals and requirements of the subtask.

2.  **Initial Exploration & Planning (Iteration 1):**
    *   This is the first attempt at creating a concrete implementation plan.
    *   Explore the codebase to identify the precise files, functions, and even specific lines of code that will need modification.
    *   Determine the intended code changes (diffs) and their locations.
    *   Gather *all* relevant details from this exploration phase.

3.  **Log the Plan:**
    *   Run `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='<detailed plan>'`.
    *   Provide the *complete and detailed* findings from the exploration phase in the prompt. Include file paths, line numbers, proposed diffs, reasoning, and any potential challenges identified. Do not omit details. The goal is to create a rich, timestamped log within the subtask's `details`.

4.  **Verify the Plan:**
    *   Run `get_task` / `task-master show <subtaskId>` again to confirm that the detailed implementation plan has been successfully appended to the subtask's details.

5.  **Begin Implementation:**
    *   Set the subtask status using `set_task_status` / `task-master set-status --id=<subtaskId> --status=in-progress`.
    *   Start coding based on the logged plan.

6.  **Refine and Log Progress (Iteration 2+):**
    *   As implementation progresses, you will encounter challenges, discover nuances, or confirm successful approaches.
    *   **Before appending new information**: Briefly review the *existing* details logged in the subtask (using `get_task` or recalling from context) to ensure the update adds fresh insights and avoids redundancy.
    *   **Regularly** use `update_subtask` / `task-master update-subtask --id=<subtaskId> --prompt='<update details>\n- What worked...\n- What didn't work...'` to append new findings.
    *   **Crucially, log:**
        *   What worked ("fundamental truths" discovered).
        *   What didn't work and why (to avoid repeating mistakes).
        *   Specific code snippets or configurations that were successful.
        *   Decisions made, especially if confirmed with user input.
        *   Any deviations from the initial plan and the reasoning.
    *   The objective is to continuously enrich the subtask's details, creating a log of the implementation journey that helps the you (and human developers) learn, adapt, and avoid repeating errors.
    *   **For learned-memories.mdc updates**: Only propose updates for high-impact learnings following the [Memory Guidelines](mdc:rules/memory.mdc#what-qualifies-as-important-to-remember). Skip trivial implementation details.

7.  **Review & Update Rules (Post-Implementation):**
    *   Once the implementation for the subtask is functionally complete, review all code changes and the relevant chat history.
    *   Check `learned-memories.mdc` for patterns that emerged during this implementation
    *   Identify any new or modified code patterns, conventions, or best practices established during the implementation.
    *   Apply the [Self-Improvement Rules](mdc:rules/self_improve.mdc#connection-with-memory-system) to determine if:
        *   Memory entries should be promoted to formal rules
        *   New patterns warrant rule creation (used 3+ times)
        *   Existing rules need updates based on new learnings
    *   Create new or update existing rules following [Cursor Rules Guidelines](mdc:rules/cursor_rules.mdc)
    *   Consider creating rules for:
        *   Architectural patterns that proved successful
        *   Anti-patterns to avoid (from failed approaches)
        *   Domain-specific conventions discovered
        *   Testing strategies that work well

8.  **Finalize Implementation:**
    *   After the primary coding for a feature or subtask is complete (but before tests are written), ensure it's logged appropriately. Then, follow the [Post-Implementation Subtask Sequence](mdc:#post-implementation-subtask-sequence).

9.  **Proceed to Next Task/Subtask:**
    *   Identify the next task or subtask (e.g., using `next_task` / `task-master next`).

## Post-Implementation Subtask Sequence

After the initial code for a feature/subtask (let's call this the "Feature Code") is implemented and considered ready for testing, the following sequence of subtasks **must** be created and completed:

1.  **Subtask: "Commit Initial [Feature/Subtask Name] Code"**
    *   **Dependencies**: Completion of the initial Feature Code implementation.
    *   **Description**: Commit the initial implementation of the Feature Code for [Feature/Subtask Name].
    *   **Details**:
        *   Stage all relevant Feature Code changes (`git add .`).
        *   Craft a commit message (e.g., `git commit -m 'feat(module): Implement initial Feature X'`).
    *   **Test Strategy**: Verify the commit has been successfully made.
    *   **Status Transition**: `pending` -> `in-progress` -> `done`.

2.  **Subtask: "Implement Tests for [Feature/Subtask Name]"**
    *   **Dependencies**: The previous "Commit Initial [Feature/Subtask Name] Code" subtask.
    *   **Description**: Write the necessary unit and/or integration tests for the [Feature/Subtask Name].
    *   **Details**:
        *   Create or modify test files (e.g., `*.test.ts`).
        *   Follow guidelines in `rules-for-writing-tests.mdc`.
        *   Log any significant decisions or challenges using `update_subtask`.
    *   **Test Strategy**: Review test code for completeness and correctness.
    *   **Status Transition**: `pending` -> `in-progress` -> `done`.

3.  **Subtask: "Commit Tests for [Feature/Subtask Name]"**
    *   **Dependencies**: The previous "Implement Tests for [Feature/Subtask Name]" subtask.
    *   **Description**: Commit the newly created or updated test files for [Feature/Subtask Name].
    *   **Details**:
        *   Stage all new/modified test files (`git add .`).
        *   Craft a commit message (e.g., `git commit -m 'test(module): Add tests for Feature X'`).
    *   **Test Strategy**: Verify the commit has been successfully made.
    *   **Status Transition**: `pending` -> `in-progress` -> `done`.

4.  **Subtask: "Run Tests and Fix Code for [Feature/Subtask Name]"**
    *   **Dependencies**: The previous "Commit Tests for [Feature/Subtask Name]" subtask.
    *   **Description**: Continuously run the associated tests for the recently implemented and committed [Feature/Subtask Name]. Iteratively debug and fix the source code (Feature Code) and/or test code until all tests pass (with a special exception for integration tests, see below).
    *   **Details**:
        *   **Test Execution Mode**: All test commands (e.g., Vitest, Jest) **must** be run with a flag that ensures they execute once and then exit, without entering "watch mode" (e.g., `vitest run`, `jest --watchAll=false`). This prevents the need for manual intervention (like pressing 'q') to stop the test runner.
        *   Log any significant issues encountered and their resolutions using `update_subtask`.
        *   Refer to `rules-for-writing-tests.mdc` for detailed guidance on running and interpreting tests.
    *   **Test Strategy**: All tests related to [Feature/Subtask Name] must pass.
        *   **Special Exception for Integration Tests**: Due to a temporary known glitch, integration tests **must still be run**. However, if they fail, **do not attempt to fix them at this stage**. The subsequent commit subtask can proceed even with failing integration tests. This exception will be removed once the underlying glitch is resolved.
    *   **Status Transition**: `pending` -> `in-progress` -> `done` (only when all tests pass, or when integration tests have been run and noted if failing, per the exception).

5.  **Subtask: "Commit Successfully Tested/Fixed Code for [Feature/Subtask Name]"**
    *   **Dependencies**: The previous "Run Tests and Fix Code" subtask for [Feature/Subtask Name].
    *   **Description**: Commit any code changes (to either Feature Code or test code) made while ensuring tests pass for [Feature/Subtask Name].
    *   **Details**:
        *   Stage all relevant code changes (`git add .`).
        *   Craft a comprehensive Git commit message (e.g., `git commit -m 'fix(module): Fix tests for Feature X'`).
        *   Consider if a Changeset is needed.
    *   **Test Strategy**: Verify the commit has been successfully made.
    *   **Status Transition**: `pending` -> `in-progress` -> `done`.

This sequence ensures that Feature Code implementation, test implementation, test execution/fixing, and their respective commits are explicit, tracked steps.

## Code Analysis & Refactoring Techniques

- **Top-Level Function Search**:
    - Useful for understanding module structure or planning refactors.
    - Use grep/ripgrep to find exported functions/constants:
      `rg "export (async function|function|const) \w+"` or similar patterns.
    - Can help compare functions between files during migrations or identify potential naming conflicts.

---
*This workflow provides a general guideline. Adapt it based on your specific project needs and team practices.*